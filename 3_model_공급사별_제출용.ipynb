{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.read_csv('./make_data/1_final_train.csv')\n",
    "test = pd.read_csv('./make_data/2_final_test.csv') \n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "weather_10 = pd.read_csv(\"./make_data/0D_weather_10_float.csv\")\n",
    "ensemble = copy.deepcopy(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gas_nmae(true_df, pred_df):\n",
    "    true = true_df.iloc[:,0].to_numpy()\n",
    "    pred = pred_df.iloc[:,0].to_numpy()\n",
    "    score = np.mean((np.abs(true-pred))/true)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1,2,3월만 학습에 사용하기 위해 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total[(total.month==1)|(total.month==2)|(total.month==3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 업체별 train, test 세트를 리스트로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list=[]\n",
    "for chain in range(7):\n",
    "    train0 = total[total.s_chain==chain]\n",
    "    test0 = test[test.s_chain==chain]\n",
    "    tlist = [train0, test0]\n",
    "    train_list.append(tlist)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 334\n",
      "[LightGBM] [Info] Number of data points in the train set: 10824, number of used features: 8\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 1922.714531\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\tvalid_0's l1: 133.513\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid_0's l1: 132.282\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 334\n",
      "[LightGBM] [Info] Number of data points in the train set: 10824, number of used features: 8\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 1652.850405\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\tvalid_0's l1: 141.4\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid_0's l1: 139.574\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 334\n",
      "[LightGBM] [Info] Number of data points in the train set: 10824, number of used features: 8\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 189.827305\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\tvalid_0's l1: 16.8335\n",
      "Early stopping, best iteration is:\n",
      "[74]\tvalid_0's l1: 16.584\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 334\n",
      "[LightGBM] [Info] Number of data points in the train set: 10824, number of used features: 8\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 1122.055651\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\tvalid_0's l1: 81.7696\n",
      "Early stopping, best iteration is:\n",
      "[56]\tvalid_0's l1: 80.6968\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 334\n",
      "[LightGBM] [Info] Number of data points in the train set: 10824, number of used features: 8\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 2389.198482\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\tvalid_0's l1: 229.225\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid_0's l1: 223.723\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 334\n",
      "[LightGBM] [Info] Number of data points in the train set: 10824, number of used features: 8\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 3076.138341\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\tvalid_0's l1: 355.721\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid_0's l1: 355.713\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 334\n",
      "[LightGBM] [Info] Number of data points in the train set: 10824, number of used features: 8\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 430.277566\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\tvalid_0's l1: 30.834\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid_0's l1: 30.3057\n",
      "[0.06695763455276367, 0.07577100435064624, 0.09077672754788377, 0.06770553315055729, 0.07925082942575538, 0.09680845473345157, 0.06881944415929518]\n",
      "score = 0.07801280398862187\n"
     ]
    }
   ],
   "source": [
    "train_years = [2013,2014,2015,2016,2017]\n",
    "val_years = [2018]\n",
    "idx = 0\n",
    "sco_list = []\n",
    "\n",
    "for train_df, test_df in train_list:\n",
    "    \n",
    "    train = train_df[train_df['year'].isin(train_years)]\n",
    "    val = train_df[train_df['year'].isin(val_years)]\n",
    "\n",
    "    features = ['time','weekday', 'no_working', 'temp', 'lunMonth', 'lunDay', 'month', '일사']\n",
    "    \n",
    "    train_x = train[features]\n",
    "    train_y = train['s_qty']\n",
    "    val_x = val[features]\n",
    "    val_y = val['s_qty']\n",
    "    test_x = test_df[features]\n",
    "\n",
    "    train_x = train_x.reset_index(drop=True)\n",
    "    train_y = train_y.reset_index(drop=True)\n",
    "    val_x = val_x.reset_index(drop=True)\n",
    "    val_y = val_y.reset_index(drop=True)\n",
    "    test_x = test_x.reset_index(drop=True)\n",
    "    \n",
    "    d_train = lgb.Dataset(train_x, train_y)\n",
    "    d_val = lgb.Dataset(val_x, val_y)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric':'mae',\n",
    "        'max_depth' : 10,\n",
    "        'seed':42\n",
    "    }\n",
    "\n",
    "    model = lgb.train(params, d_train, 500, d_val, verbose_eval=50, early_stopping_rounds=10)\n",
    "    joblib.dump(model, f'./models/lgbm_model_{idx}.pkl')\n",
    "    pred2 = model.predict(val_x)\n",
    "    pred_df = pd.DataFrame(pred2)\n",
    "    valy_df = pd.DataFrame(val_y)\n",
    "    nmae_score = gas_nmae(valy_df, pred_df)\n",
    "    sco_list.append(nmae_score)\n",
    "    preds = model.predict(test_x)\n",
    "    submission.iloc[0+(2160*idx):2160+(2160*idx), 1] = preds\n",
    "    idx += 1\n",
    "    \n",
    "print(sco_list)\n",
    "print('score =', np.array(sco_list).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble['lgbm'] = submission['공급량']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:44:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { metric } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1908.75781\teval-rmse:2056.78516\n",
      "[200]\ttrain-rmse:92.51829\teval-rmse:182.37749\n",
      "[229]\ttrain-rmse:88.49469\teval-rmse:183.29411\n",
      "[14:44:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { metric } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1642.56238\teval-rmse:1831.23413\n",
      "[200]\ttrain-rmse:75.11555\teval-rmse:193.22826\n",
      "[255]\ttrain-rmse:69.36401\teval-rmse:195.57648\n",
      "[14:44:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { metric } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:183.59943\teval-rmse:177.74800\n",
      "[200]\ttrain-rmse:10.98854\teval-rmse:20.74912\n",
      "[258]\ttrain-rmse:10.10164\teval-rmse:20.91256\n",
      "[14:44:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { metric } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:1109.69128\teval-rmse:1216.22644\n",
      "[200]\ttrain-rmse:49.34002\teval-rmse:111.67231\n",
      "[245]\ttrain-rmse:45.21676\teval-rmse:113.18938\n",
      "[14:44:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { metric } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:2369.43994\teval-rmse:2697.75732\n",
      "[200]\ttrain-rmse:105.47222\teval-rmse:321.73169\n",
      "[260]\ttrain-rmse:96.03806\teval-rmse:323.97519\n",
      "[14:44:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { metric } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:3016.85107\teval-rmse:3464.86523\n",
      "[200]\ttrain-rmse:130.13145\teval-rmse:432.21930\n",
      "[257]\ttrain-rmse:121.96390\teval-rmse:434.27499\n",
      "[14:44:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { metric } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:426.37412\teval-rmse:464.43484\n",
      "[200]\ttrain-rmse:19.68980\teval-rmse:42.28907\n",
      "[234]\ttrain-rmse:18.77078\teval-rmse:42.68131\n",
      "[0.07165625180033777, 0.08095517639088509, 0.09225300313626124, 0.07260949182547478, 0.08250356777789318, 0.1003576822890701, 0.0727411125619242]\n",
      "0.08186804082597805\n"
     ]
    }
   ],
   "source": [
    "train_years = [2013,2014,2015,2016,2017]\n",
    "val_years = [2018]\n",
    "idx = 0\n",
    "sco_list = []\n",
    "\n",
    "for train_df, test_df in train_list:\n",
    "    \n",
    "    train = train_df[train_df['year'].isin(train_years)]\n",
    "    val = train_df[train_df['year'].isin(val_years)]\n",
    "\n",
    "    features = ['time','weekday', 'no_working', 'temp', 'lunMonth', 'lunDay', 'month', '일사']\n",
    "\n",
    "    train_x = train[features]\n",
    "    train_y = train['s_qty']\n",
    "    val_x = val[features]\n",
    "    val_y = val['s_qty']\n",
    "    test_x = test_df[features]\n",
    "\n",
    "    train_x = train_x.reset_index(drop=True)\n",
    "    train_y = train_y.reset_index(drop=True)\n",
    "    val_x = val_x.reset_index(drop=True)\n",
    "    val_y = val_y.reset_index(drop=True)\n",
    "    test_x = test_x.reset_index(drop=True)\n",
    "\n",
    "    dtrain = xgb.DMatrix(data=train_x, label = train_y)\n",
    "    dval = xgb.DMatrix(data=val_x, label = val_y)\n",
    "    wlist = [(dtrain, 'train'), (dval,'eval')]\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': 0.05, \n",
    "        'objective': 'reg:squarederror',\n",
    "        'metric':'mae', \n",
    "        'seed':42\n",
    "    }\n",
    "\n",
    "    model = xgb.train( params, dtrain, 2000, evals=wlist, verbose_eval=200, early_stopping_rounds=100)\n",
    "    joblib.dump(model, f'./models/xgb_model_{idx}.pkl')\n",
    "    val_x2 = xgb.DMatrix(val_x)\n",
    "    pred2 = model.predict(val_x2)\n",
    "    pred_df = pd.DataFrame(pred2)\n",
    "    valy_df = pd.DataFrame(val_y)\n",
    "    nmae_score = gas_nmae(valy_df, pred_df)\n",
    "    sco_list.append(nmae_score)\n",
    "    test_x = xgb.DMatrix(test_x)\n",
    "    preds = model.predict(test_x)\n",
    "    submission.iloc[0+(2160*idx):2160+(2160*idx), 1] = preds\n",
    "    idx += 1\n",
    "    \n",
    "print(sco_list)\n",
    "print(np.array(sco_list).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble['xgb'] = submission['공급량']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTree 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "[0.07234832780755159, 0.07550939226977922, 0.09328408451340006, 0.06844908710062438, 0.08223177774072636, 0.0966670699094651, 0.07023285789493533]\n",
      "0.07981751389092599\n"
     ]
    }
   ],
   "source": [
    "train_years = [2013,2014,2015,2016,2017]\n",
    "val_years = [2018]\n",
    "idx = 0\n",
    "sco_list = []\n",
    "\n",
    "for train_df, test_df in train_list:\n",
    "    \n",
    "    train = train_df[train_df['year'].isin(train_years)]\n",
    "    val = train_df[train_df['year'].isin(val_years)]\n",
    "\n",
    "    features = ['time','weekday', 'no_working', 'temp', 'lunMonth', 'lunDay', 'month', '일사']\n",
    "\n",
    "    train_x = train[features]\n",
    "    train_y = train['s_qty']\n",
    "    val_x = val[features]\n",
    "    val_y = val['s_qty']\n",
    "    test_x = test_df[features]\n",
    "\n",
    "    train_x = train_x.reset_index(drop=True)\n",
    "    train_y = train_y.reset_index(drop=True)\n",
    "    val_x = val_x.reset_index(drop=True)\n",
    "    val_y = val_y.reset_index(drop=True)\n",
    "    test_x = test_x.reset_index(drop=True)\n",
    "\n",
    "    model = ExtraTreesRegressor(max_depth=12, n_estimators=130)\n",
    "    model.fit(train_x, train_y)\n",
    "    joblib.dump(model, f'./models/ET_model_{idx}.pkl')\n",
    "    pred2 = model.predict(val_x)\n",
    "\n",
    "    pred_df = pd.DataFrame(pred2)\n",
    "    valy_df = pd.DataFrame(val_y)\n",
    "    nmae_score = gas_nmae(valy_df, pred_df)\n",
    "    sco_list.append(nmae_score)\n",
    "    preds = model.predict(test_x)\n",
    "    print(type(preds))\n",
    "    submission.iloc[0+(2160*idx):2160+(2160*idx), 1] = preds\n",
    "    idx += 1\n",
    "    \n",
    "print(sco_list)\n",
    "print(np.array(sco_list).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble['extree'] = submission['공급량']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07128998129225687, 0.079127065062904, 0.09457833951692249, 0.07217597776345915, 0.08651107790749729, 0.09777222363860316, 0.07212822211155143]\n",
      "0.08194041247045634\n"
     ]
    }
   ],
   "source": [
    "train_years = [2013,2014,2015,2016,2017]\n",
    "val_years = [2018]\n",
    "idx = 0\n",
    "sco_list = []\n",
    "\n",
    "for train_df, test_df in train_list:\n",
    "    \n",
    "    train = train_df[train_df['year'].isin(train_years)]\n",
    "    val = train_df[train_df['year'].isin(val_years)]\n",
    "\n",
    "    features = ['time','weekday', 'no_working', 'temp', 'lunMonth', 'lunDay', 'month', '일사']\n",
    "\n",
    "    train_x = train[features]\n",
    "    train_y = train['s_qty']\n",
    "    val_x = val[features]\n",
    "    val_y = val['s_qty']\n",
    "    test_x = test_df[features]\n",
    "\n",
    "    train_x = train_x.reset_index(drop=True)\n",
    "    train_y = train_y.reset_index(drop=True)\n",
    "    val_x = val_x.reset_index(drop=True)\n",
    "    val_y = val_y.reset_index(drop=True)\n",
    "    test_x = test_x.reset_index(drop=True)\n",
    "    \n",
    "    model = RandomForestRegressor(max_depth=12, n_estimators=100, random_state=0)\n",
    "    model.fit(train_x, train_y)\n",
    "    joblib.dump(model, f'./models/rf_model_{idx}.pkl')\n",
    "    pred2 = model.predict(val_x)\n",
    "\n",
    "    pred_df = pd.DataFrame(pred2)\n",
    "    valy_df = pd.DataFrame(val_y)\n",
    "    nmae_score = gas_nmae(valy_df, pred_df)\n",
    "    sco_list.append(nmae_score)\n",
    "    preds = model.predict(test_x)\n",
    "    submission.iloc[0+(2160*idx):2160+(2160*idx), 1] = preds\n",
    "    idx += 1\n",
    "    \n",
    "print(sco_list)\n",
    "print(np.array(sco_list).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble['rf'] = submission['공급량']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 결과 조합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['공급량'] = (ensemble['extree'] + ensemble['rf'] + ensemble['lgbm'] + ensemble['xgb']) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자|시간|구분</th>\n",
       "      <th>공급량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 01 A</td>\n",
       "      <td>2318.512307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 02 A</td>\n",
       "      <td>2019.532236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 03 A</td>\n",
       "      <td>2003.164477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 04 A</td>\n",
       "      <td>2053.181453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 05 A</td>\n",
       "      <td>2115.151596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15115</th>\n",
       "      <td>2019-03-31 20 H</td>\n",
       "      <td>392.147604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15116</th>\n",
       "      <td>2019-03-31 21 H</td>\n",
       "      <td>392.665583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15117</th>\n",
       "      <td>2019-03-31 22 H</td>\n",
       "      <td>372.319881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15118</th>\n",
       "      <td>2019-03-31 23 H</td>\n",
       "      <td>320.386734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15119</th>\n",
       "      <td>2019-03-31 24 H</td>\n",
       "      <td>295.238848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15120 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              일자|시간|구분          공급량\n",
       "0      2019-01-01 01 A  2318.512307\n",
       "1      2019-01-01 02 A  2019.532236\n",
       "2      2019-01-01 03 A  2003.164477\n",
       "3      2019-01-01 04 A  2053.181453\n",
       "4      2019-01-01 05 A  2115.151596\n",
       "...                ...          ...\n",
       "15115  2019-03-31 20 H   392.147604\n",
       "15116  2019-03-31 21 H   392.665583\n",
       "15117  2019-03-31 22 H   372.319881\n",
       "15118  2019-03-31 23 H   320.386734\n",
       "15119  2019-03-31 24 H   295.238848\n",
       "\n",
       "[15120 rows x 2 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submit/3_submission_ensemble.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
